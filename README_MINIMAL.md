# ğŸ¤– Ollama Chatbot - Minimal Deployment Ready

Tamamen Ã¼cretsiz, production-ready Ollama chatbot sistemi.

## ğŸ“‚ Proje YapÄ±sÄ±

```
ollama-chatbot/
â”œâ”€â”€ app_minimal.py          # Flask API (Backend)
â”œâ”€â”€ requirements_minimal.txt # Python dependencies
â”œâ”€â”€ Dockerfile_minimal      # Docker container
â””â”€â”€ frontend/               # React frontend (opsiyonel)
    â”œâ”€â”€ package.json
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ App.js
    â”‚   â”œâ”€â”€ App.css
    â”‚   â””â”€â”€ index.js
    â””â”€â”€ public/
        â””â”€â”€ index.html
```

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Yerel Test

#### Backend (Flask API):
```bash
# 1. Ollama'yÄ± baÅŸlat
ollama serve
ollama pull llama3

# 2. Python dependencies
pip install -r requirements_minimal.txt

# 3. Flask API'yi Ã§alÄ±ÅŸtÄ±r
python app_minimal.py
```

**Web arayÃ¼zÃ¼:** http://localhost:5000

#### API Test:
```bash
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Merhaba, nasÄ±lsÄ±n?"}'
```

#### React Frontend (Opsiyonel):
```bash
cd frontend
npm install
npm start
```

**React arayÃ¼zÃ¼:** http://localhost:3000

### 2. Docker ile Ã‡alÄ±ÅŸtÄ±rma

```bash
# Backend container
docker build -f Dockerfile_minimal -t ollama-chatbot .
docker run -p 5000:5000 -e OLLAMA_HOST=http://host.docker.internal:11434 ollama-chatbot

# Test
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Docker test"}'
```

## ğŸŒ Ãœcretsiz Deployment SeÃ§enekleri

### Option 1: Railway (Ã–nerilen)
1. https://railway.app â†’ GitHub ile giriÅŸ
2. "New Project" â†’ "Deploy from GitHub repo"
3. Environment Variables:
   ```
   OLLAMA_HOST=http://YOUR_SERVER_IP:11434
   PORT=5000
   ```
4. Deploy!

### Option 2: Render
1. https://render.com â†’ GitHub connect
2. "New Web Service" â†’ Bu repo
3. Build Command: `pip install -r requirements_minimal.txt`
4. Start Command: `gunicorn --bind 0.0.0.0:$PORT app_minimal:app`

### Option 3: Oracle Cloud Free Tier
```bash
# Oracle VM'de (Ubuntu)
sudo apt update
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3

# Flask uygulamasÄ±nÄ± kur
git clone https://github.com/iremzeytinozu/local-llm.git
cd local-llm
pip install -r requirements_minimal.txt

# Gunicorn ile Ã§alÄ±ÅŸtÄ±r
gunicorn --bind 0.0.0.0:5000 app_minimal:app
```

## ğŸ”§ API Endpoints

| Endpoint | Method | AÃ§Ä±klama |
|----------|--------|----------|
| `/` | GET | Web arayÃ¼zÃ¼ |
| `/chat` | POST | Ana chat endpoint |
| `/api` | GET | API bilgileri |
| `/health` | GET | SaÄŸlÄ±k kontrolÃ¼ |
| `/models` | GET | Mevcut modeller |

### Chat API KullanÄ±mÄ±:

**Request:**
```json
POST /chat
{
  "message": "Merhaba, nasÄ±lsÄ±n?"
}
```

**Response:**
```json
{
  "response": "Merhaba! Ben bir AI asistanÄ±yÄ±m ve Ã§ok iyiyim, teÅŸekkÃ¼rler..."
}
```

## ğŸ¯ Environment Variables

| Variable | Default | AÃ§Ä±klama |
|----------|---------|----------|
| `OLLAMA_HOST` | `http://localhost:11434` | Ollama server adresi |
| `MODEL_NAME` | `llama3` | KullanÄ±lacak model |
| `PORT` | `5000` | Flask app portu |
| `FLASK_ENV` | `production` | Flask environment |

## ğŸ”’ Production OptimizasyonlarÄ±

### 1. Gunicorn Configuration:
```bash
gunicorn --bind 0.0.0.0:5000 --workers 2 --timeout 120 app_minimal:app
```

### 2. Nginx Reverse Proxy:
```nginx
server {
    listen 80;
    server_name your-domain.com;
    
    location / {
        proxy_pass http://localhost:5000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### 3. Rate Limiting:
```python
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

limiter = Limiter(
    app,
    key_func=get_remote_address,
    default_limits=["100 per hour"]
)

@app.route("/chat", methods=["POST"])
@limiter.limit("10 per minute")
def chat():
    # ...
```

## ğŸ“Š Monitoring

### Health Check:
```bash
curl http://your-app.com/health
```

### Logs:
```bash
# Railway
railway logs

# Docker
docker logs container_name

# Systemd
journalctl -u your-app
```

## ğŸš¨ Troubleshooting

### Common Issues:

1. **Ollama Connection Error:**
   ```bash
   # Ollama durumu kontrol
   ollama list
   curl http://localhost:11434/api/tags
   ```

2. **CORS HatasÄ±:**
   - Flask-CORS yÃ¼klÃ¼ olduÄŸundan emin olun
   - Frontend'in proxy ayarlarÄ±nÄ± kontrol edin

3. **Port HatasÄ±:**
   - `PORT` environment variable'Ä±nÄ± kontrol edin
   - BaÅŸka servislerin portu kullanmadÄ±ÄŸÄ±ndan emin olun

### Debug Mode:
```bash
FLASK_ENV=development python app_minimal.py
```

## ğŸ’° Maliyet Analizi

### Tamamen Ãœcretsiz SeÃ§enek:
- **Oracle Cloud:** Always Free (VM)
- **Railway:** $5/ay kredi (yeterli)
- **Domain:** Railway subdomain (Ã¼cretsiz)
- **SSL:** Otomatik (Ã¼cretsiz)

**Toplam: 0 TL/ay**

### Profesyonel SeÃ§enek:
- **DigitalOcean Droplet:** $6/ay
- **Custom Domain:** $10-15/yÄ±l
- **Cloudflare:** Ãœcretsiz plan

**Toplam: ~$8/ay**

## ğŸ”„ CI/CD Pipeline

### GitHub Actions (Opsiyonel):
```yaml
name: Deploy to Railway
on:
  push:
    branches: [main]
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Deploy to Railway
        run: |
          npx @railway/cli deploy
```

## ğŸ“š Next Steps

1. **Model Optimization:** Daha kÃ¼Ã§Ã¼k modeller iÃ§in quantization
2. **Caching:** Redis ile response cache
3. **Authentication:** JWT token sistemi
4. **Analytics:** Conversation tracking
5. **Multi-model:** Model switching API

---

**ğŸ‰ Congratulations!** 

ArtÄ±k tamamen Ã¼cretsiz, production-ready bir AI chatbot sisteminiz var!

**Demo:** https://your-app.railway.app
**API Docs:** https://your-app.railway.app/api
